{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIOMEDIN 260/RAD260: Problem Set 3 - Mammogram Project\n",
    "\n",
    "## Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name 1:\n",
    "\n",
    "Kristen Anderson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name 2:\n",
    "\n",
    "Ryan Crowley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Breast cancer has the highest incidence and second highest mortality rate for women in the US.\n",
    "\n",
    "Your task is to utilize machine learning to study mammograms in any way you want (e.g. classification, segmentation) as long as you justify why it is useful to do whatever it is you want to do. Turning in a deep dream assignment using mammograms might be amusing, for example, but not so useful to patients. That being said, choose something that interests you. As the adage goes, \"do what you love, and you’ll never have to work another day in your life, at least in BMI 260.\"\n",
    "\n",
    "Treat this as a mini-project. We highly encourage working with 1 other person, possibly someone in your main project team. \n",
    "\n",
    "In addition to the mammograms themselves, the dataset includes \"ground-truth\" segmentations and `mass_case_description_train_set.csv`, which contains metadata information about mass shapes, mass margins, assessment numbers, pathology diagnoses, and subtlety in the data. Take some time to research what all of these different fields mean and how you might utilize them in your work. You dont need to use all of what is provided to you.\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "1. Use the ROI’s or segmentations to extract features, and then train a classifier based on those features using the algorithms presented to you in the machine learning lectures (doesn't need to use deep learning).\n",
    "\n",
    "2. Use convolutional neural networks. Feel free to use any of the code we went over in class or use your own (custom code, sklearn, keras, Tensorflow etc.). If you dont want to place helper functions and classes into this notebook, place them in a `.py` file in the same folder called `helperfunctions.py` and import them into this notebook.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is here:\n",
    "\n",
    "https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n",
    "\n",
    "## Grading and Submission\n",
    "\n",
    "This assignment has 3 components: code, figures (outputs/analyses of your code), and a write-up detailing your mini-project. You will be graded on these categories.\n",
    "\n",
    "If you're OK with Python or R, please place all three parts into this notebook/.Rmd file that we have provided where indicated. We have written template sections for you to follow for simplicity/completeness. When you're done, save as a `.pdf` (please knit to `.pdf` if you are using `.Rmd`, or knit to `.html` and use a browser's \"Print\" function to convert to `.pdf`).\n",
    "\n",
    "If you don't like Python OR R, we will allow you to use a different language, but please turn your assignment in with: 1) a folder with all your code, 2) a folder with all your figures, and 3) a `.tex`/`.doc`/`.pdf` file with a write-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiating between Malignant and Benign Masses in Mammograms using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe what you are doing and why it matters to patients using at least one citation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the VGG-16 CNN to diagnose benign and malignant tumors in mammograms. In doing so, we could help doctors diagnose cancer at earlier stages through discovering malignant tumors that may otherwise be missed. CNNs have been shown to classify mammogram images with an overall sensitivity of 96% and a 0.99 AUC [1]. In contrast, doctors acurately diagnose breast cancer through mammography in about 78% of their patients, with diagnostic accuracy rising to about 83% when female patients are older than 50 [2].\n",
    "\n",
    "The difference between these scores suggests both a significant need for technological aid as well as the CNN's capacity to effectively improve diagnosis. The chance that a woman will die from breast cancer is about 1 in 38, so reducing this risk could save many women's lives [3]. Early diagnosis gives women the opportunity to receive proper medical care when fighting cancer.\n",
    "\n",
    "1. https://pubmed.ncbi.nlm.nih.gov/31838610/\n",
    "2. https://www.uchealth.org/today/how-accurate-are-mammograms/\n",
    "3. https://www.cancer.org/cancer/breast-cancer/about/how-common-is-breast-cancer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Describe the relevant statistics of the data. How were the images taken? How were they labeled? What is the class balance and majority classifier accuracy? How will you divide the data into testing, training and validation sets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are taken using low-dose X-ray mammography as the imgaging technique. These images were collected from a variety of sources including Massachusetts General Hopsital and Washington University of St. Louis School of Medicine. The images are labeled with diverse information including ROIs for calcifications and masses as well as BI-RADS information about the mass shape, mass margin, breast density, and other data. There also exists other metadata that include the patient age, date of study, scanner used to digitize, and resolution of the image along other data. Cases with abnormalities also contain extra information including the type of abnormality. We will be focusing upon the masses. There is a training set of 653 benign abnormalities and 621 malignant abnormalities. There also exists a testing set with 189 benign abnormalities and 156 malignant abnormalities. As such, the majority classifer accuracy on the test set is .5478. We will keep the groups sorted as given and will not perform any hyperparameter tuning so a validation set will not be neccessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Describe your data pipeline (how is the data scrubbed, normalized, stored, and fed to the model for training?).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pipeline that we developed first used the csv files to link individual patient ID's with the associated diagnosis of whether the mass was benign or malignant. We then looped through all of the individual ROI images in the \"data_fixed_crop_w_mask\" folder to assign them into either the training malignant folder or the training benign folder. We also cut the images down in size to 224 x 224 pixels as this is the required input for our version of VGG-16. We chose to cut the images down by taking the center 224 x 224 pixels and cutting off the border around each image. We then performed the same data preprocessing with all of the individual ROI images in the \"test\" folder to assign them either to the test malignant folder or the test benign folder and cut them down in size as well to 224 x 224 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Explain how the model you chose works alongside the code for it. Add at least one technical citation to give credit where credit is due.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import skimage\n",
    "import h5py\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import keras\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary that links patient id to outcome\n",
    "dictionary = {};\n",
    "with open('mass_case_description_train_set-1.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "  \n",
    "        dictionary[key] = row[9]\n",
    "with open('mass_case_description_test_set-1.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "        dictionary[key] = row[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training data set\n",
    "directory = \"data_fixed_crop_w_mask\"\n",
    "for folder in os.listdir(directory):\n",
    "    path = directory + \"/\" + folder;\n",
    "    sum = 0\n",
    "    for filename in os.listdir(path):\n",
    "        sum += 1\n",
    "        fullFilePath = directory + \"/\" + folder + \"/\" + filename;\n",
    "        f = h5py.File(fullFilePath, \"r\")\n",
    "        datasetNames = [n for n in f.keys()]\n",
    "        a_group = f['data']\n",
    "        second = a_group[:,:,1]\n",
    "        finalImage = second[16:240,16:240]\n",
    "        \n",
    "        patient_ID =  \"P_\" + folder;\n",
    "        if patient_ID in dictionary:\n",
    "            for row in range(0,224):\n",
    "                for col in range(0,224):\n",
    "                    if finalImage[row,col] < .5:\n",
    "                        finalImage[row,col] = 0\n",
    "                    else:\n",
    "                        finalImage[row,col] = 255\n",
    "            im = Image.fromarray(finalImage)\n",
    "            im = im.convert(\"L\")\n",
    "\n",
    "            if dictionary[patient_ID] == \"MALIGNANT\":\n",
    "                im.save('trainSorted/malignant/'+patient_ID+'_'+str(sum)+'.png', 'png')\n",
    "            else:\n",
    "                im.save('trainSorted/benign/'+patient_ID+'_'+str(sum)+'.png', 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the testing dataset\n",
    "directory = \"test\"\n",
    "for folder in os.listdir(directory):\n",
    "    path = directory + \"/\" + folder;\n",
    "    if path != 'test/.DS_Store': \n",
    "        sum = 0\n",
    "        for filename in os.listdir(path):\n",
    "            sum += 1\n",
    "            fullFilePath = directory + \"/\" + folder + \"/\" + filename;\n",
    "            f = h5py.File(fullFilePath, \"r\")\n",
    "            datasetNames = [n for n in f.keys()]\n",
    "            a_group = f['data']\n",
    "            second = a_group[:,:,1]\n",
    "            finalImage = second[16:240,16:240]\n",
    "\n",
    "            patient_ID =  \"P_\" + folder;\n",
    "            if patient_ID in dictionary:\n",
    "                for row in range(0,224):\n",
    "                    for col in range(0,224):\n",
    "                        if finalImage[row,col] < .5:\n",
    "                            finalImage[row,col] = 0\n",
    "                        else:\n",
    "                            finalImage[row,col] = 255\n",
    "                im = Image.fromarray(finalImage)\n",
    "                im = im.convert(\"L\")\n",
    "\n",
    "                if dictionary[patient_ID] == \"MALIGNANT\":\n",
    "                    im.save('testSorted/malignant/'+patient_ID+'_'+str(sum)+'.png', 'png')\n",
    "                else:\n",
    "                    im.save('testSorted/benign/'+patient_ID+'_'+str(sum)+'.png', 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1274 images belonging to 2 classes.\n",
      "Found 345 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Load the data so that it can be used in the model\n",
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory=\"/Users/ryancrowley/Desktop/BMI 260/trainSorted/\",target_size=(224,224))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory=\"/Users/ryancrowley/Desktop/BMI 260/testSorted/\", target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ryancrowley/opt/anaconda3/envs/bmi2602/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 119,554,050\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryancrowley/opt/anaconda3/envs/bmi2602/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Edit the model so that it works for this application\n",
    "vggmodel = VGG16(weights='imagenet', include_top=True)\n",
    "for layers in (vggmodel.layers)[:19]:\n",
    "    layers.trainable = False\n",
    "X= vggmodel.layers[-2].output\n",
    "predictions = Dense(2, activation=\"softmax\")(X)\n",
    "model_final = Model(input = vggmodel.input, output = predictions)\n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ryancrowley/opt/anaconda3/envs/bmi2602/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 58s 29s/step - loss: 0.9766 - accuracy: 0.5469 - val_loss: 1.1089 - val_accuracy: 0.3750\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryancrowley/opt/anaconda3/envs/bmi2602/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/Users/ryancrowley/opt/anaconda3/envs/bmi2602/lib/python3.6/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 46s 23s/step - loss: 1.1332 - accuracy: 0.5156 - val_loss: 0.6396 - val_accuracy: 0.5625\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 45s 23s/step - loss: 1.0413 - accuracy: 0.5312 - val_loss: 0.9555 - val_accuracy: 0.6250\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 48s 24s/step - loss: 1.0534 - accuracy: 0.5000 - val_loss: 1.0931 - val_accuracy: 0.4375\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 48s 24s/step - loss: 1.1376 - accuracy: 0.5156 - val_loss: 0.8782 - val_accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 57s 29s/step - loss: 1.0228 - accuracy: 0.5625 - val_loss: 1.3707 - val_accuracy: 0.4688\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 52s 26s/step - loss: 1.1867 - accuracy: 0.5156 - val_loss: 0.7215 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 54s 27s/step - loss: 0.9830 - accuracy: 0.4531 - val_loss: 1.1456 - val_accuracy: 0.3125\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 52s 26s/step - loss: 0.8931 - accuracy: 0.5000 - val_loss: 0.8169 - val_accuracy: 0.4688\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 50s 25s/step - loss: 0.9900 - accuracy: 0.5312 - val_loss: 0.6293 - val_accuracy: 0.7188\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 57s 28s/step - loss: 0.7348 - accuracy: 0.5312 - val_loss: 0.9188 - val_accuracy: 0.3200\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 56s 28s/step - loss: 0.8073 - accuracy: 0.5172 - val_loss: 0.6479 - val_accuracy: 0.6562\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 63s 31s/step - loss: 0.8000 - accuracy: 0.5469 - val_loss: 0.6431 - val_accuracy: 0.7188\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 62s 31s/step - loss: 0.7490 - accuracy: 0.4844 - val_loss: 0.5815 - val_accuracy: 0.6250\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 62s 31s/step - loss: 0.7521 - accuracy: 0.5781 - val_loss: 0.6333 - val_accuracy: 0.6875\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 62s 31s/step - loss: 0.8650 - accuracy: 0.5000 - val_loss: 0.7076 - val_accuracy: 0.5938\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 67s 33s/step - loss: 0.6864 - accuracy: 0.6094 - val_loss: 0.7003 - val_accuracy: 0.6250\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 65s 32s/step - loss: 0.8665 - accuracy: 0.4844 - val_loss: 0.7134 - val_accuracy: 0.5312\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 70s 35s/step - loss: 0.8577 - accuracy: 0.4688 - val_loss: 0.7529 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 66s 33s/step - loss: 0.8835 - accuracy: 0.4531 - val_loss: 0.6320 - val_accuracy: 0.7812\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 64s 32s/step - loss: 0.6822 - accuracy: 0.5781 - val_loss: 0.7100 - val_accuracy: 0.5938\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 56s 28s/step - loss: 0.7655 - accuracy: 0.5156 - val_loss: 1.0915 - val_accuracy: 0.4400\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 66s 33s/step - loss: 0.8529 - accuracy: 0.5781 - val_loss: 1.1303 - val_accuracy: 0.5312\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 63s 32s/step - loss: 0.7588 - accuracy: 0.5938 - val_loss: 0.8501 - val_accuracy: 0.5938\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 64s 32s/step - loss: 0.8278 - accuracy: 0.5156 - val_loss: 0.7272 - val_accuracy: 0.5312\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 63s 32s/step - loss: 0.6497 - accuracy: 0.6562 - val_loss: 0.9167 - val_accuracy: 0.5625\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 63s 32s/step - loss: 0.8112 - accuracy: 0.4688 - val_loss: 0.6698 - val_accuracy: 0.6250\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 59s 29s/step - loss: 0.7218 - accuracy: 0.5469 - val_loss: 0.7890 - val_accuracy: 0.4688\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 62s 31s/step - loss: 0.9085 - accuracy: 0.4688 - val_loss: 0.6969 - val_accuracy: 0.5938\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 61s 31s/step - loss: 0.8420 - accuracy: 0.5625 - val_loss: 0.6588 - val_accuracy: 0.6250\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 67s 34s/step - loss: 0.8283 - accuracy: 0.5469 - val_loss: 0.7104 - val_accuracy: 0.4688\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 83s 42s/step - loss: 0.7923 - accuracy: 0.5312 - val_loss: 0.7863 - val_accuracy: 0.5625\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 58s 29s/step - loss: 0.7008 - accuracy: 0.6406 - val_loss: 0.6828 - val_accuracy: 0.5600\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 63s 32s/step - loss: 0.8157 - accuracy: 0.5781 - val_loss: 0.5340 - val_accuracy: 0.7812\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 60s 30s/step - loss: 0.9292 - accuracy: 0.5625 - val_loss: 0.6491 - val_accuracy: 0.5938\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 64s 32s/step - loss: 0.7527 - accuracy: 0.4844 - val_loss: 1.0024 - val_accuracy: 0.4062\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 64s 32s/step - loss: 0.7818 - accuracy: 0.5312 - val_loss: 0.6312 - val_accuracy: 0.6562\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 62s 31s/step - loss: 0.7321 - accuracy: 0.5312 - val_loss: 0.6390 - val_accuracy: 0.6562\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 57s 29s/step - loss: 0.6417 - accuracy: 0.5862 - val_loss: 0.6466 - val_accuracy: 0.5938\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 57s 28s/step - loss: 0.6580 - accuracy: 0.6406 - val_loss: 0.4824 - val_accuracy: 0.8125\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 65s 32s/step - loss: 0.6681 - accuracy: 0.5312 - val_loss: 0.4995 - val_accuracy: 0.6875\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 63s 31s/step - loss: 0.6274 - accuracy: 0.6719 - val_loss: 0.8575 - val_accuracy: 0.6250\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 59s 30s/step - loss: 0.6581 - accuracy: 0.6562 - val_loss: 1.0343 - val_accuracy: 0.5625\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 55s 28s/step - loss: 0.6803 - accuracy: 0.6406 - val_loss: 0.6494 - val_accuracy: 0.7200\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 60s 30s/step - loss: 0.6053 - accuracy: 0.6406 - val_loss: 0.7640 - val_accuracy: 0.5938\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 70s 35s/step - loss: 0.7167 - accuracy: 0.5781 - val_loss: 0.6595 - val_accuracy: 0.5938\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 62s 31s/step - loss: 0.6118 - accuracy: 0.6875 - val_loss: 0.9666 - val_accuracy: 0.5625\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 52s 26s/step - loss: 0.8231 - accuracy: 0.5312 - val_loss: 0.7211 - val_accuracy: 0.5625\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 51s 25s/step - loss: 0.6488 - accuracy: 0.6250 - val_loss: 0.6212 - val_accuracy: 0.5938\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 54s 27s/step - loss: 0.6494 - accuracy: 0.6406 - val_loss: 0.5563 - val_accuracy: 0.7500\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 55s 27s/step - loss: 0.5626 - accuracy: 0.7031 - val_loss: 0.4763 - val_accuracy: 0.8750\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 51s 25s/step - loss: 0.6735 - accuracy: 0.5690 - val_loss: 0.6543 - val_accuracy: 0.6250\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 61s 31s/step - loss: 0.6192 - accuracy: 0.6250 - val_loss: 0.6946 - val_accuracy: 0.5625\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 61s 30s/step - loss: 0.7069 - accuracy: 0.5469 - val_loss: 0.6040 - val_accuracy: 0.6875\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 63s 32s/step - loss: 0.5583 - accuracy: 0.7656 - val_loss: 0.4797 - val_accuracy: 0.7200\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 56s 28s/step - loss: 0.9033 - accuracy: 0.4688 - val_loss: 0.6656 - val_accuracy: 0.5938\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 54s 27s/step - loss: 0.6271 - accuracy: 0.6562 - val_loss: 0.7704 - val_accuracy: 0.5312\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 56s 28s/step - loss: 0.6380 - accuracy: 0.6094 - val_loss: 0.5655 - val_accuracy: 0.6875\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 57s 28s/step - loss: 0.6500 - accuracy: 0.5781 - val_loss: 0.7270 - val_accuracy: 0.5625\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 54s 27s/step - loss: 0.6955 - accuracy: 0.5781 - val_loss: 0.6462 - val_accuracy: 0.6562\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 57s 28s/step - loss: 0.5963 - accuracy: 0.6562 - val_loss: 0.5686 - val_accuracy: 0.8438\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 57s 29s/step - loss: 0.6022 - accuracy: 0.6406 - val_loss: 0.6336 - val_accuracy: 0.5312\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 52s 26s/step - loss: 0.6829 - accuracy: 0.5469 - val_loss: 0.5114 - val_accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 50s 25s/step - loss: 0.5477 - accuracy: 0.7188 - val_loss: 0.6305 - val_accuracy: 0.6875\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 42s 21s/step - loss: 0.6170 - accuracy: 0.6250 - val_loss: 0.8560 - val_accuracy: 0.5938\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 43s 22s/step - loss: 0.5890 - accuracy: 0.6562 - val_loss: 0.6577 - val_accuracy: 0.6400\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.6554 - accuracy: 0.6094 - val_loss: 0.5147 - val_accuracy: 0.6562\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 43s 21s/step - loss: 0.6523 - accuracy: 0.7031 - val_loss: 0.5319 - val_accuracy: 0.6562\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.6854 - accuracy: 0.5938 - val_loss: 0.6693 - val_accuracy: 0.6562\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 46s 23s/step - loss: 0.7482 - accuracy: 0.4844 - val_loss: 0.6290 - val_accuracy: 0.6250\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 42s 21s/step - loss: 0.6402 - accuracy: 0.5938 - val_loss: 0.6544 - val_accuracy: 0.5938\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.7904 - accuracy: 0.5000 - val_loss: 0.6496 - val_accuracy: 0.5938\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.6489 - accuracy: 0.6875 - val_loss: 0.8400 - val_accuracy: 0.6250\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 0.7659 - accuracy: 0.5862 - val_loss: 0.6970 - val_accuracy: 0.6250\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 43s 21s/step - loss: 0.7145 - accuracy: 0.6094 - val_loss: 0.5657 - val_accuracy: 0.6875\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 0.6240 - accuracy: 0.6562 - val_loss: 0.6535 - val_accuracy: 0.7188\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 0.6581 - accuracy: 0.5938 - val_loss: 0.8955 - val_accuracy: 0.5200\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 0.5118 - accuracy: 0.7656 - val_loss: 0.5952 - val_accuracy: 0.6875\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 45s 22s/step - loss: 0.6832 - accuracy: 0.5938 - val_loss: 0.7398 - val_accuracy: 0.5312\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 45s 23s/step - loss: 0.5510 - accuracy: 0.8281 - val_loss: 0.4768 - val_accuracy: 0.7812\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 0.6016 - accuracy: 0.6250 - val_loss: 0.5220 - val_accuracy: 0.7188\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 50s 25s/step - loss: 0.4890 - accuracy: 0.8281 - val_loss: 0.4631 - val_accuracy: 0.7188\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 45s 23s/step - loss: 0.4926 - accuracy: 0.8125 - val_loss: 0.8964 - val_accuracy: 0.5625\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 47s 23s/step - loss: 0.4770 - accuracy: 0.7969 - val_loss: 0.9050 - val_accuracy: 0.5938\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.5302 - accuracy: 0.7031 - val_loss: 0.6994 - val_accuracy: 0.7188\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 46s 23s/step - loss: 0.5929 - accuracy: 0.6562 - val_loss: 0.5831 - val_accuracy: 0.7812\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 51s 26s/step - loss: 0.6179 - accuracy: 0.5938 - val_loss: 0.5834 - val_accuracy: 0.6875\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.5781 - accuracy: 0.6875 - val_loss: 0.5542 - val_accuracy: 0.7200\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 46s 23s/step - loss: 0.6098 - accuracy: 0.6562 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 45s 23s/step - loss: 0.5260 - accuracy: 0.7344 - val_loss: 0.5900 - val_accuracy: 0.6875\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 0.5307 - accuracy: 0.7241 - val_loss: 0.5826 - val_accuracy: 0.6875\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 42s 21s/step - loss: 0.5755 - accuracy: 0.6719 - val_loss: 0.4501 - val_accuracy: 0.8125\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 46s 23s/step - loss: 0.5424 - accuracy: 0.7188 - val_loss: 0.4050 - val_accuracy: 0.8438\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 48s 24s/step - loss: 0.5647 - accuracy: 0.7188 - val_loss: 0.7907 - val_accuracy: 0.6875\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 50s 25s/step - loss: 0.6025 - accuracy: 0.6250 - val_loss: 0.6001 - val_accuracy: 0.6875\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 48s 24s/step - loss: 0.5679 - accuracy: 0.7031 - val_loss: 0.6046 - val_accuracy: 0.6562\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.5328 - accuracy: 0.7031 - val_loss: 0.5363 - val_accuracy: 0.7188\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 47s 23s/step - loss: 0.6346 - accuracy: 0.6406 - val_loss: 0.6785 - val_accuracy: 0.5625\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 43s 22s/step - loss: 0.6390 - accuracy: 0.6562 - val_loss: 0.6503 - val_accuracy: 0.7200\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 46s 23s/step - loss: 0.5998 - accuracy: 0.6406 - val_loss: 0.5149 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "#Run the training and validation of the model\n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=40, verbose=1, mode='auto')\n",
    "model_final.fit_generator(generator= traindata, steps_per_epoch= 2, epochs= 100, validation_data= testdata, validation_steps=1, callbacks=[checkpoint,early])\n",
    "model_final.save_weights(\"vgg16_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data preprocessing steps, we then loaded the data into the correct format by using ImageDataGenerator. From here, we then imported the VGG-16 model that was pretrained on the ImageNet dataset. We then altered the structure of the network such that it only outputs malignant or benign instead of the 1000 different categories it would output in the past. We also altered the network such that the first 19 layers were frozen as part of the transfer learning approach. This approach is overall similar to a transfer learning approach using previously to differentiate between COVID-19 and viral pneumonia with a great deal of success [1]. \n",
    "\n",
    "1.https://arxiv.org/abs/2003.13145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. There are many ways to do training. Take us through how you do it (e.g. \"We used early stopping and stopped when validation loss increased twice in a row.\").**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used an early stopping criteria where we would stop if the validation accuracy did not change over the course of two consecutive epochs. Additionally, we trained using a total of 100 epochs with 2 steps per epoch. Our learning rate was eta = 0.0001, and we used Stochastic Gradient Descent with momentum = .9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Make a figure displaying your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "height = [.50939,.61637,.55651,.60192,.62513,.65012,.66319,.63638,.68764,.71263]\n",
    "bars = ('1-10', '11-20', '21-30', '31-40', '41-50','51-60','61-70','71-80','81-90','91-100')\n",
    "y_pos = np.arange(len(bars))\n",
    " \n",
    "# Create bars\n",
    "plt.bar(y_pos, height)\n",
    " \n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "plt.xlabel(\"Epoch During Training\")\n",
    "\n",
    "plt.ylabel(\"Average Validation Accuracy\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we asssessed the average validation accuracy over 10 different intervals during the training process. Overall, we found that the validation accuracy appeared to increase over the course of the training indicating that the model was working as intended. Of note, the model outperforms the majority classifier accuracy of .5478"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Discuss pros and cons of your method and what you might have done differently now that you've tried or would try if you had more time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note, this method achieved a moderate level of success with our final 10 iterations of the model averaging a validation accuracy of .71263  Our model appeared to show some success in differentiating between benign and malignant tumors. This is likely largely due to the fact that CNNs are powerful for image analysis and transfer learning can be an effective method for quickly and efficiently training a model. Some possible limitations of our approach included our relatively small amount of data, the fact that no textural features could be calculated due to the fact that our input images were segmentations, and a relatively short training period. Additionally, accuracy appeared to fluctuate significantly indicating that the model was quite variable. Finally, the model that we developed may not easily fit into clinical workflows due to the need to determine the boundaries of the ROI. If we had had more time and more computational power, we could have tried to use more images to train the dataset and trained the model over the course of more epochs. Of course, there would then exist the possibility of overfitting so if we chose to train for more epochs we would also include additional methods to regularize the model. Overall, our approach shows some promise but also leaves significant room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will not be graded on the performance of your model. You'll only be graded on the scientific soundness of your claims, methodology, evaluation (i.e. fair but insightful statistics), and discussion of the strengths and shortcomings of what you tried. Feel free to reuse some of the code you are/will be using for your projects. The write-up doesn't need to be long (~1 page will suffice), but please cite at least one clinical paper and one technical paper (1 each in questions 1 and 4 at least, and more if needed).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
